{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"Gutenberg Text Recommendations.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMWCpWnYGpFmPGMwmkrSnw1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Scraping"],"metadata":{"id":"EJsbsYDok32k"}},{"cell_type":"code","execution_count":null,"source":["from bs4 import BeautifulSoup\r\n","import requests\r\n","import re\r\n","import pandas as pd\r\n","from math import ceil"],"outputs":[],"metadata":{"id":"65r1BBKbM9HE"}},{"cell_type":"markdown","source":[],"metadata":{"id":"ad7Nz0mr6riA"}},{"cell_type":"code","execution_count":null,"source":["BASE_URL = \"https://www.gutenberg.org\"\r\n","PARSER_NAME = 'lxml'"],"outputs":[],"metadata":{"id":"jp3Az-H2xIEI"}},{"cell_type":"markdown","source":["Code which creates a Pandas dataframe where:\n","  - Each dictionary within the list contains the information; more specifically, the title, author, and link to the HTML version of the books are stored in each dictionary.\n","  - The order of these dictionaries within the list is the same order as their 'ranking' on the Gutenberg website.\n","  - Books which do not have an HTML version (e.g. math textbooks or picture books which contain large amounts of content which isn't plain text) are ignored.\n","  - The created dictionary will contain exactly `num_2_get` books."],"metadata":{"id":"PbYYNTcipClI"}},{"cell_type":"code","execution_count":null,"source":["def process_booklinks(bl_element, book_id, num_2_take):\r\n","  # Create urls for corresponding books:\r\n","  book_urls = [BASE_URL + \"/ebooks/\" + str(id) for id in book_id]\r\n","  # Get soups for each link:\r\n","  soup_list = [BeautifulSoup(requests.get(url).text, PARSER_NAME) for url in book_urls]\r\n","  # Find URL extensions associated with \"HTML (Original)\" text links on page:\r\n","  html_ext = [s.find('a', text=\"Read this book online: HTML (original)\") for s in soup_list]\r\n","  # Get lists of Authors, Titles, and links for books with HTML versions available:\r\n","  authors, titles, urls = [], [], []\r\n","  # Iterate over each book's unique description page:\r\n","  for i, ext in enumerate(html_ext):\r\n","    # Only add book if it has an HTML version of the book is available AND\r\n","    # if we still haven't taken num_2_take books:\r\n","    if ext is not None and len(authors) < num_2_take:\r\n","      # Get title from list of booklinks:\r\n","      title_i = bl_element[i].find(class_=\"title\").contents[0]\r\n","      # Some titles have /r character at end - remove them if so:\r\n","      title_i = title_i[:-2] if title_i[-2:] == '\\r' else title_i\r\n","      titles.append(title_i)\r\n","      # Get author from list of booklinks - need to check if book has a non-empty\r\n","      # author tag:\r\n","      author_i = bl_element[i].find(class_=\"subtitle\")\r\n","      author_i = author_i.contents[0] if author_i is not None else \"Anonymous\"\r\n","      authors.append(author_i)\r\n","      # Get URL of HTML version of text:\r\n","      urls.append(BASE_URL + str(ext['href']))\r\n","  # Return these lists:\r\n","  return (authors, titles, urls)\r\n","\r\n","def get_book_info(num_2_get):\r\n","  # Generate URLs of webpages to visit by appending extension to end of base directory:\r\n","  start_url = BASE_URL + \"/ebooks/search/?sort_order=downloads\"\r\n","  # Initialise loop variables:\r\n","  current_url = start_url\r\n","  num_books = 0 \r\n","  num_books_checked = 0\r\n","  # Initialise lists to store author, title, and url information for each book:\r\n","  author_list, title_list, url_list = [], [], []\r\n","  # Get the number of books we've specified:\r\n","  while num_books < num_2_get:\r\n","    # Get soup of current URL:\r\n","    soup = BeautifulSoup(requests.get(current_url).text, PARSER_NAME)\r\n","    # Get booklink elements off this page:\r\n","    current_el = [x for x in soup.find_all(class_='booklink')]\r\n","    # Determine the maximum number of books we need to take from this page:\r\n","    num_2_take = min(len(current_el), num_2_get - num_books)\r\n","    # Get book 'id' of each book on page:\r\n","    current_id = [x.a['href'].split('/')[-1] for x in current_el]\r\n","    # Get the Title, Author and URLs for each book which has an HTML version:\r\n","    (authors_i, titles_i, urls_i) = process_booklinks(current_el, current_id, num_2_take)\r\n","    # Append to our 'grand' lists:\r\n","    author_list += authors_i\r\n","    title_list += titles_i\r\n","    url_list += urls_i\r\n","    # Update how many books we've collected and checked from this page:\r\n","    num_books += len(authors_i)\r\n","    num_books_checked += len(current_id)\r\n","    # Update our current_url:\r\n","    current_url = start_url + f\"&start_index={num_books_checked+1}\"\r\n","  # Convert these lists into a Pandas dataframe:\r\n","  book_df = pd.DataFrame.from_dict({'title': title_list,\r\n","                                    'author': author_list,\r\n","                                    'url': url_list})\r\n","  return book_df"],"outputs":[],"metadata":{"id":"Eh3dxDrrLtco"}},{"cell_type":"code","execution_count":null,"source":["num_2_get = 100\r\n","book_df = get_book_info(num_2_get)"],"outputs":[],"metadata":{"id":"Fk2aX4EhSYYo"}},{"cell_type":"markdown","source":["Now that we have a dictionary which contains links to the HTML version of each book we want to scrape, let's now actually scrape the text of each book:"],"metadata":{"id":"6lVjCLMHqkdM"}},{"cell_type":"code","execution_count":null,"source":["def get_text(book_df):\r\n","  # Create regex to 'clean' text we'll scrape:\r\n","  clean_regex = re.compile('(<.*?>)|Ã‚|([^ \\w\\.])')\r\n","  # Initialise list to store text:\r\n","  text_list = []\r\n","  # Iterate over urls in dataframe:\r\n","  for url in book_df['url']:\r\n","    # Get soup of page:\r\n","    soup = BeautifulSoup(requests.get(url).text, PARSER_NAME)\r\n","    # Initialise string to contain text:\r\n","    text_i = ''\r\n","    # Iterate over paragraphs on page and add to text:\r\n","    for p in soup.find_all('p'):\r\n","      text_i += re.sub(clean_regex, '', p.get_text())\r\n","    # Append i'th text to list:\r\n","    text_list.append(text_i)\r\n","  # Add text list as new column to dataframe:\r\n","  book_df['text'] = text_list\r\n","  return book_df\r\n","\r\n","text_df = get_text(book_df)"],"outputs":[],"metadata":{"id":"fpQjzeXPBd3I"}},{"cell_type":"markdown","source":["Save to CSV:"],"metadata":{"id":"La2B9ArJX5Cs"}},{"cell_type":"code","execution_count":null,"source":["text_df.to_csv('text.csv')"],"outputs":[],"metadata":{"id":"LwGwrcuTNCRE"}}]}