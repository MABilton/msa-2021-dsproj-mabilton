{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20adddcf",
   "metadata": {
    "papermill": {
     "duration": 0.013161,
     "end_time": "2021-08-31T05:07:24.880775",
     "exception": false,
     "start_time": "2021-08-31T05:07:24.867614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Kaggle Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259fbe15",
   "metadata": {
    "papermill": {
     "duration": 0.012725,
     "end_time": "2021-08-31T05:07:24.906348",
     "exception": false,
     "start_time": "2021-08-31T05:07:24.893623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa424cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T05:07:24.935936Z",
     "iopub.status.busy": "2021-08-31T05:07:24.934783Z",
     "iopub.status.idle": "2021-08-31T05:07:33.673818Z",
     "shell.execute_reply": "2021-08-31T05:07:33.672988Z",
     "shell.execute_reply.started": "2021-08-31T05:06:17.902905Z"
    },
    "papermill": {
     "duration": 8.754921,
     "end_time": "2021-08-31T05:07:33.674013",
     "exception": false,
     "start_time": "2021-08-31T05:07:24.919092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PyTorch imports:\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, SequentialSampler\n",
    "\n",
    "# HuggingFace imports:\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Data manipulation imports:\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f4eac",
   "metadata": {
    "papermill": {
     "duration": 0.011765,
     "end_time": "2021-08-31T05:07:33.698206",
     "exception": false,
     "start_time": "2021-08-31T05:07:33.686441",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Input Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ee0156",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T05:07:33.727646Z",
     "iopub.status.busy": "2021-08-31T05:07:33.726947Z",
     "iopub.status.idle": "2021-08-31T05:07:33.729561Z",
     "shell.execute_reply": "2021-08-31T05:07:33.730089Z",
     "shell.execute_reply.started": "2021-08-31T05:06:25.795323Z"
    },
    "papermill": {
     "duration": 0.020078,
     "end_time": "2021-08-31T05:07:33.730275",
     "exception": false,
     "start_time": "2021-08-31T05:07:33.710197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define input parameters:\n",
    "MAX_LEN = 250\n",
    "NUM_PRED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08fa4d00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T05:07:33.769181Z",
     "iopub.status.busy": "2021-08-31T05:07:33.767916Z",
     "iopub.status.idle": "2021-08-31T05:07:33.771482Z",
     "shell.execute_reply": "2021-08-31T05:07:33.770828Z",
     "shell.execute_reply.started": "2021-08-31T05:06:25.804278Z"
    },
    "papermill": {
     "duration": 0.029071,
     "end_time": "2021-08-31T05:07:33.771621",
     "exception": false,
     "start_time": "2021-08-31T05:07:33.742550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Processes each chunk of tokenized text so it can be 'understood' by BERT:\n",
    "def process_chunks(id_chunk, mask_chunk, max_len, start_id=101, end_id=102):\n",
    "  # Add start and stop IDs:\n",
    "  id_chunk = torch.cat([torch.tensor([start_id]), id_chunk, torch.tensor([end_id])])\n",
    "  mask_chunk = torch.cat([torch.ones(1), mask_chunk, torch.ones(1)])\n",
    "  # Pad chunks so that they're all of the same length:\n",
    "  if len(id_chunk) < max_len:\n",
    "    pad_len = max_len - len(id_chunk)\n",
    "    id_chunk = torch.cat([id_chunk, torch.zeros(pad_len)])\n",
    "    mask_chunk = torch.cat([mask_chunk, torch.zeros(pad_len)])\n",
    "  return (id_chunk, mask_chunk)\n",
    "\n",
    "# Divides up each piece of text into chunks of size max_len then tokenizes them:\n",
    "def tokenise_chunks(features, tokenizer, max_len):\n",
    "  X_id, X_mask = [], []\n",
    "  for text in features[\"excerpt\"]:\n",
    "    id_list, mask_list = [], []\n",
    "    # Encode piece of text:\n",
    "    tokens = tokenizer.encode_plus(text, add_special_tokens=False, return_tensors='pt')\n",
    "    # Split ID and attention mask into chunks of size (max_len-2) - note we need to add\n",
    "    # a start and end token to these chunks:\n",
    "    ids = tokens[\"input_ids\"][0].split(max_len-2)\n",
    "    masks = tokens[\"attention_mask\"][0].split(max_len-2)\n",
    "    # Add start and end token to each chunk:\n",
    "    for id_chunk, mask_chunk in zip(ids, masks):\n",
    "      id_chunk, mask_chunk = process_chunks(id_chunk, mask_chunk, max_len)\n",
    "      id_list.append(id_chunk), mask_list.append(mask_chunk)\n",
    "    X_id.append(torch.stack(id_list, dim=0))\n",
    "    X_mask.append(torch.stack(mask_list, dim=0))\n",
    "  # Pad list of IDs and Masks so that they're now both stored in a single tensor\n",
    "  # of dimensions (batch size × num of chunks × num of mask/id values):\n",
    "  X_id = pad_sequence(X_id, batch_first=True, padding_value=0)\n",
    "  X_mask = pad_sequence(X_mask, batch_first=True, padding_value=0)\n",
    "  X_id, X_mask = torch.as_tensor(X_id, dtype=torch.int64), torch.as_tensor(X_mask, dtype=torch.int64)\n",
    "  return (X_id, X_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94fc8e26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T05:07:33.798837Z",
     "iopub.status.busy": "2021-08-31T05:07:33.798219Z",
     "iopub.status.idle": "2021-08-31T05:07:33.805208Z",
     "shell.execute_reply": "2021-08-31T05:07:33.804659Z",
     "shell.execute_reply.started": "2021-08-31T05:06:25.821586Z"
    },
    "papermill": {
     "duration": 0.021569,
     "end_time": "2021-08-31T05:07:33.805365",
     "exception": false,
     "start_time": "2021-08-31T05:07:33.783796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset class\n",
    "class ReadabilityData(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_i = self.data[idx]\n",
    "        dict_i = {\"id\": item_i[0],\n",
    "                  \"mask\": item_i[1], \n",
    "                  \"wt\": item_i[2]}\n",
    "        return dict_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96019a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T05:07:33.836831Z",
     "iopub.status.busy": "2021-08-31T05:07:33.836165Z",
     "iopub.status.idle": "2021-08-31T05:07:33.838245Z",
     "shell.execute_reply": "2021-08-31T05:07:33.838707Z",
     "shell.execute_reply.started": "2021-08-31T05:06:25.836114Z"
    },
    "papermill": {
     "duration": 0.021392,
     "end_time": "2021-08-31T05:07:33.838888",
     "exception": false,
     "start_time": "2021-08-31T05:07:33.817496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For creating the test dataset:\n",
    "#    - dataset output includes X_id, X_mask, and y\n",
    "#    - X_id and X_mask are ([batch size*num of chunks] × num of mask/id values)\n",
    "#    - y is ([batch size*num of chunks] × num of outputs)\n",
    "def create_test_dataset(test_features, tokenizer, max_len):\n",
    "  # Split text into chunks and tokenise those chunks:\n",
    "  X_id, X_mask = tokenise_chunks(test_features, tokenizer, max_len)\n",
    "  # Compute weightings for the sentence chunks:\n",
    "  X_wts = torch.sum(X_mask, axis=2, keepdim=True)/torch.sum(X_mask, axis=(1,2), keepdim=True)\n",
    "  X_wts = X_wts.reshape(X_mask.shape[0:-1])\n",
    "  X_wts = torch.as_tensor(X_wts, dtype=torch.float32)\n",
    "  # Place tensors into TensorDataset object:\n",
    "  test_dataset = TensorDataset(X_id, X_mask, X_wts)\n",
    "  # Create Dataset object:\n",
    "  test_dataset = ReadabilityData(test_dataset)\n",
    "  return test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41e85a",
   "metadata": {
    "papermill": {
     "duration": 0.011822,
     "end_time": "2021-08-31T05:07:33.863124",
     "exception": false,
     "start_time": "2021-08-31T05:07:33.851302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e517107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T05:07:33.890219Z",
     "iopub.status.busy": "2021-08-31T05:07:33.889562Z",
     "iopub.status.idle": "2021-08-31T05:07:33.897651Z",
     "shell.execute_reply": "2021-08-31T05:07:33.898197Z",
     "shell.execute_reply.started": "2021-08-31T05:06:25.851666Z"
    },
    "papermill": {
     "duration": 0.023146,
     "end_time": "2021-08-31T05:07:33.898372",
     "exception": false,
     "start_time": "2021-08-31T05:07:33.875226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertModel():\n",
    "    def __init__(self, load_dir, num_pred):\n",
    "        self.num_pred = num_pred\n",
    "        # Load fine-tuned Bert model:\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained(load_dir)\n",
    "        \n",
    "    def predict(self, id, att_mask, wt):\n",
    "        with torch.no_grad():\n",
    "            if self.model.training:\n",
    "              self.model.eval()\n",
    "            # Compute input and output shapes:\n",
    "            in_shape = (id.shape[0]*id.shape[1], id.shape[2])\n",
    "            out_shape = id.shape[0:-1] + (self.num_pred,)\n",
    "            id, att_mask = id.reshape(in_shape), att_mask.reshape(in_shape)\n",
    "            # Make prediction with BERT model:\n",
    "            logits = self.model(input_ids=id, attention_mask=att_mask)[\"logits\"]\n",
    "            # Convert output to NumPy array so that we can use nan_to_num:\n",
    "            logits = logits.reshape(out_shape).numpy()\n",
    "            # Take weighted-average of BERT predictions for each chunk of text:\n",
    "            pred = np.einsum(\"ij,ijk->ik\", wt, np.nan_to_num(logits))\n",
    "            pred = pred.squeeze()\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6918d",
   "metadata": {
    "papermill": {
     "duration": 0.012135,
     "end_time": "2021-08-31T05:07:33.922782",
     "exception": false,
     "start_time": "2021-08-31T05:07:33.910647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9517d00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T05:07:33.951225Z",
     "iopub.status.busy": "2021-08-31T05:07:33.950560Z",
     "iopub.status.idle": "2021-08-31T05:07:37.479757Z",
     "shell.execute_reply": "2021-08-31T05:07:37.480343Z",
     "shell.execute_reply.started": "2021-08-31T05:06:25.864415Z"
    },
    "papermill": {
     "duration": 3.545118,
     "end_time": "2021-08-31T05:07:37.480609",
     "exception": false,
     "start_time": "2021-08-31T05:07:33.935491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dir = \"../input/finetuned-bert-readability/best_model\"\n",
    "bert_model = BertModel(load_dir, NUM_PRED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9c0114",
   "metadata": {
    "papermill": {
     "duration": 0.012212,
     "end_time": "2021-08-31T05:07:37.508195",
     "exception": false,
     "start_time": "2021-08-31T05:07:37.495983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "576999ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T05:07:37.537291Z",
     "iopub.status.busy": "2021-08-31T05:07:37.536624Z",
     "iopub.status.idle": "2021-08-31T05:07:39.357340Z",
     "shell.execute_reply": "2021-08-31T05:07:39.356472Z",
     "shell.execute_reply.started": "2021-08-31T05:06:31.121683Z"
    },
    "papermill": {
     "duration": 1.836973,
     "end_time": "2021-08-31T05:07:39.357532",
     "exception": false,
     "start_time": "2021-08-31T05:07:37.520559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load test data:\n",
    "test_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n",
    "test_df = test_df.loc[:,[\"id\", \"excerpt\"]]\n",
    "\n",
    "# Tokenize the text in this test dataset:\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"../input/finetuned-bert-readability/tokenizer\")\n",
    "test_dataset = create_test_dataset(test_df, tokenizer, MAX_LEN)\n",
    "\n",
    "# Drop text now that it's tokenized:\n",
    "test_df.drop(columns=\"excerpt\", inplace=True)\n",
    "\n",
    "# Place tokenized dataset into Dataloader:\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=32,\n",
    "                             shuffle=False)\n",
    "\n",
    "# Make predictions:\n",
    "pred_list = np.array([])\n",
    "for d in test_dataloader:\n",
    "    pred = bert_model.predict(d[\"id\"],\n",
    "                              d[\"mask\"],\n",
    "                              d[\"wt\"])\n",
    "    pred_list = np.concatenate((pred_list, pred), axis=0)\n",
    "\n",
    "# Add predictions to test dataframe:\n",
    "test_df[\"target\"] = np.array(pred_list)\n",
    "# Save ids and predictions to csv file:\n",
    "test_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12466640",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T05:07:39.390839Z",
     "iopub.status.busy": "2021-08-31T05:07:39.390148Z",
     "iopub.status.idle": "2021-08-31T05:07:39.410850Z",
     "shell.execute_reply": "2021-08-31T05:07:39.410226Z",
     "shell.execute_reply.started": "2021-08-31T05:06:33.319383Z"
    },
    "papermill": {
     "duration": 0.039636,
     "end_time": "2021-08-31T05:07:39.411024",
     "exception": false,
     "start_time": "2021-08-31T05:07:39.371388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.152462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>0.084220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.219362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.287382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.792509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>-0.871888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>0.273297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.152462\n",
       "1  f0953f0a5  0.084220\n",
       "2  0df072751 -0.219362\n",
       "3  04caf4e0c -2.287382\n",
       "4  0e63f8bea -1.792509\n",
       "5  12537fe78 -0.871888\n",
       "6  965e592c0  0.273297"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586fe1c7",
   "metadata": {
    "papermill": {
     "duration": 0.012248,
     "end_time": "2021-08-31T05:07:39.436339",
     "exception": false,
     "start_time": "2021-08-31T05:07:39.424091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25.88473,
   "end_time": "2021-08-31T05:07:41.653491",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-31T05:07:15.768761",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
