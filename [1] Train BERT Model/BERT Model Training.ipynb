{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT Model Training.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9472d303c871431e9cc3045e44c82548":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d3ba04387bd9408b871fe1c26256dc2e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1cf12f9bb68744db8893624d59220d17","IPY_MODEL_dafcc78ff42a475c8acd5028cc4de62b","IPY_MODEL_7e009e826592405d908b695b97c7b2b2"]}},"d3ba04387bd9408b871fe1c26256dc2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1cf12f9bb68744db8893624d59220d17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_33e51f405e85422fbb38aaa80471ab69","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0a5187bad2be4836bb61e33dbb954124"}},"dafcc78ff42a475c8acd5028cc4de62b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9e1bb382865f47fd90279785edbbfde8","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9845213625634f72b74eac8b56694c07"}},"7e009e826592405d908b695b97c7b2b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7d1ab4d9b2cf43e49d196bd290fab5d4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 1.40MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6f6a49c62bed4c78bbc86a31afc9bf08"}},"33e51f405e85422fbb38aaa80471ab69":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0a5187bad2be4836bb61e33dbb954124":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9e1bb382865f47fd90279785edbbfde8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9845213625634f72b74eac8b56694c07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7d1ab4d9b2cf43e49d196bd290fab5d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6f6a49c62bed4c78bbc86a31afc9bf08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9f579559fda840bda85e810b14dc9dd2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_42100cf6c2544f2b8067f030354fd847","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3e3dce1339a14634af2147cadcd65b92","IPY_MODEL_0cffc75fb2f64ee78741729903513163","IPY_MODEL_f8a074bb5fcc4f7aa83697482258d938"]}},"42100cf6c2544f2b8067f030354fd847":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3e3dce1339a14634af2147cadcd65b92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d154208f501b47009f8c6b9829a28f45","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fe65ba75c019472b948a66e47abfcbaa"}},"0cffc75fb2f64ee78741729903513163":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3a648ad9bf74444390a367ecb888f38a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_86d8f136a3424dd8aee25b7275e4505d"}},"f8a074bb5fcc4f7aa83697482258d938":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2628c85da2bc4850b27f4116bd4c85d4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 613B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_88cbd2ee30a24a138ea8b7bf07db0be4"}},"d154208f501b47009f8c6b9829a28f45":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fe65ba75c019472b948a66e47abfcbaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3a648ad9bf74444390a367ecb888f38a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"86d8f136a3424dd8aee25b7275e4505d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2628c85da2bc4850b27f4116bd4c85d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"88cbd2ee30a24a138ea8b7bf07db0be4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"91fb426a45ae49569a3edee4ec5dc218":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2345212dea5d4715b54fe574e37e6917","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ceaa0d400a924fe7b576603285575b97","IPY_MODEL_3b365168723c455a9f4d5869207c41d1","IPY_MODEL_58ed7fd2b0bb43e6be27a9ede8989f79"]}},"2345212dea5d4715b54fe574e37e6917":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ceaa0d400a924fe7b576603285575b97":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_13078c196e604ebd9beb5dbe68e3157d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_749e49d3105942318a845fd602f56dcd"}},"3b365168723c455a9f4d5869207c41d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f890f884bc8a4f2b886f14c8436a5510","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a4054bad6224449eb69f4d640194f5e8"}},"58ed7fd2b0bb43e6be27a9ede8989f79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_18df3698cc8a4573a107361a7bdfbf4a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 2.10MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2944a5daa34b4545be62c962743c658f"}},"13078c196e604ebd9beb5dbe68e3157d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"749e49d3105942318a845fd602f56dcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f890f884bc8a4f2b886f14c8436a5510":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a4054bad6224449eb69f4d640194f5e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"18df3698cc8a4573a107361a7bdfbf4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2944a5daa34b4545be62c962743c658f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1504f065ce0e4cad94113ba38cd7f403":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_22960adcd67244a7b98e1e9de30ebd7b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b2d3d3b6edcf430cae5ea26cba7e4e56","IPY_MODEL_da2c18ab7bc94d84b43f1c3a59ccfe63","IPY_MODEL_35c5c4eba9544968868ed21d2fe3edcd"]}},"22960adcd67244a7b98e1e9de30ebd7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b2d3d3b6edcf430cae5ea26cba7e4e56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0f6d9cfed92c4c1cb11a1470fc5428ba","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2d577f26b7e24ac19058b0e0259c7c66"}},"da2c18ab7bc94d84b43f1c3a59ccfe63":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_99845ae85f304e02bc8d7c2062e7f3dc","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":483,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":483,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1f305e00a68d4db39ffd8bd9645516d9"}},"35c5c4eba9544968868ed21d2fe3edcd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_226cba960ddb4bc381c61118304344f0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 483/483 [00:00&lt;00:00, 9.40kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3a3f7d1c983f4414b4ee6a1140bcf15d"}},"0f6d9cfed92c4c1cb11a1470fc5428ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2d577f26b7e24ac19058b0e0259c7c66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"99845ae85f304e02bc8d7c2062e7f3dc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1f305e00a68d4db39ffd8bd9645516d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"226cba960ddb4bc381c61118304344f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3a3f7d1c983f4414b4ee6a1140bcf15d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8b0655fd2a424848bfc3d45ff3e35d4f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9de5818d7f2647889c3754cc2dac929a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8e45b83d7e074972978e5e2fa5ae0bdf","IPY_MODEL_576056fc267a479584dbee4b8f4cc21b","IPY_MODEL_7034090027094112a97a35088231fdd6"]}},"9de5818d7f2647889c3754cc2dac929a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8e45b83d7e074972978e5e2fa5ae0bdf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_26b98202154c4e6f938a43409388af90","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2a7eac42b70e4474b0c64ba3ac4e94fe"}},"576056fc267a479584dbee4b8f4cc21b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9b12a71d3d834a1e92bc9e41a4fc7348","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":267967963,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":267967963,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8f7e3c3b5cd641e78c5532d012f3f564"}},"7034090027094112a97a35088231fdd6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d9e6fc59641b424eb2f0a9594e21bdb8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 268M/268M [00:05&lt;00:00, 50.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0f57abe5f74c4bbd9c89a47be270a58a"}},"26b98202154c4e6f938a43409388af90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2a7eac42b70e4474b0c64ba3ac4e94fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9b12a71d3d834a1e92bc9e41a4fc7348":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8f7e3c3b5cd641e78c5532d012f3f564":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d9e6fc59641b424eb2f0a9594e21bdb8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0f57abe5f74c4bbd9c89a47be270a58a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"gf-8xTCJujaA"},"source":["# Overview\n","\n","This notebook details my approach to solving the [Kaggle Readability Competition](https://www.kaggle.com/c/commonlitreadabilityprize/overview). More specifically, this notebook details how I finetuned one of [HuggingFace's DistilBert networks](https://huggingface.co/transformers/model_doc/distilbert.html) to estimate the readability of pieces of text. \n","\n","At a 'high level', finetuning just refers to adjusting the weights of a network has already been trained on a much larger data set (but for a task and/or on a data set which is different from the one we're currently interested in) by training that same network on our much smaller data set. The key intuition behind why this works is that the network is able to *transfer* what it learnt from the large data set to doing well on our much smaller training data set.\n"]},{"cell_type":"markdown","metadata":{"id":"JyRCH1KCS7Pk"},"source":["# Set-Up\n","\n","\n","Before doing anything else, let's first install the required packages for this notebook:"]},{"cell_type":"code","metadata":{"id":"-6_TxPNnvWCo"},"source":["!pip install -r ./requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rQNNfyW0v2cE"},"source":["Next, let's perform our imports:"]},{"cell_type":"code","metadata":{"id":"kfutG9hOS850"},"source":["# PyTorch imports:\n","import torch\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torch.utils import data \n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import TensorDataset\n","\n","# HuggingFace imports:\n","import transformers\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n","\n","# Data manipulation imports:\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Plotting imports:\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Misc imports:\n","import itertools\n","import random\n","import time\n","import shutil\n","from math import inf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1G_uM_VetE6g"},"source":["Let's set the verbosity of the HuggingFace library so that it only prints 'Critical' messages:"]},{"cell_type":"code","metadata":{"id":"8HGD8euOtHA4"},"source":["transformers.logging.set_verbosity(transformers.logging.CRITICAL)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EgPh7CNS7o-Q"},"source":["Let's set our PyTorch device to be a GPU (if one is available):"]},{"cell_type":"code","metadata":{"id":"-zQbnCNr7q_8"},"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EF7h3_Z4sT-U"},"source":["For reproducability, we'll set our random seeds:"]},{"cell_type":"code","metadata":{"id":"wgmCXPz1sVSs"},"source":["RANDOM_SEED = 42\n","torch.manual_seed(RANDOM_SEED)\n","random.seed(RANDOM_SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oHzJbcLI7l0s"},"source":["Finally, we'll define the name of the DistilBert model we'll use from the HuggingFace library:"]},{"cell_type":"code","metadata":{"id":"e0VEc_f07rfM"},"source":["MODEL_NAME = \"distilbert-base-uncased\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_xg64Asz556c"},"source":["# Load in Data and Tokenizer\n","\n","Let's now load in the data (i.e. `train.csv`) - we'll throw away everything which *isn't* the text excerpt and the target value:"]},{"cell_type":"code","metadata":{"id":"vByv4NHk7RK0"},"source":["# Load data:\n","text_dir = \"./train.csv\"\n","text_df = pd.read_csv(text_dir)\n","text_df = text_df.loc[:,\"excerpt\":\"standard_error\"]\n","\n","# Split loaded data into features X and labels y:\n","features, labels = text_df.loc[:,\"excerpt\"].to_frame(), text_df.loc[:,\"target\"].to_frame()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I7WErDWtCm_o"},"source":["Next, let's create the *tokenizer* we'll use to convert our excerpts of text into numerical ID vectors which can be understood by BERT:"]},{"cell_type":"code","metadata":{"id":"e7cbTjwiCouk","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["9472d303c871431e9cc3045e44c82548","d3ba04387bd9408b871fe1c26256dc2e","1cf12f9bb68744db8893624d59220d17","dafcc78ff42a475c8acd5028cc4de62b","7e009e826592405d908b695b97c7b2b2","33e51f405e85422fbb38aaa80471ab69","0a5187bad2be4836bb61e33dbb954124","9e1bb382865f47fd90279785edbbfde8","9845213625634f72b74eac8b56694c07","7d1ab4d9b2cf43e49d196bd290fab5d4","6f6a49c62bed4c78bbc86a31afc9bf08","9f579559fda840bda85e810b14dc9dd2","42100cf6c2544f2b8067f030354fd847","3e3dce1339a14634af2147cadcd65b92","0cffc75fb2f64ee78741729903513163","f8a074bb5fcc4f7aa83697482258d938","d154208f501b47009f8c6b9829a28f45","fe65ba75c019472b948a66e47abfcbaa","3a648ad9bf74444390a367ecb888f38a","86d8f136a3424dd8aee25b7275e4505d","2628c85da2bc4850b27f4116bd4c85d4","88cbd2ee30a24a138ea8b7bf07db0be4","91fb426a45ae49569a3edee4ec5dc218","2345212dea5d4715b54fe574e37e6917","ceaa0d400a924fe7b576603285575b97","3b365168723c455a9f4d5869207c41d1","58ed7fd2b0bb43e6be27a9ede8989f79","13078c196e604ebd9beb5dbe68e3157d","749e49d3105942318a845fd602f56dcd","f890f884bc8a4f2b886f14c8436a5510","a4054bad6224449eb69f4d640194f5e8","18df3698cc8a4573a107361a7bdfbf4a","2944a5daa34b4545be62c962743c658f","1504f065ce0e4cad94113ba38cd7f403","22960adcd67244a7b98e1e9de30ebd7b","b2d3d3b6edcf430cae5ea26cba7e4e56","da2c18ab7bc94d84b43f1c3a59ccfe63","35c5c4eba9544968868ed21d2fe3edcd","0f6d9cfed92c4c1cb11a1470fc5428ba","2d577f26b7e24ac19058b0e0259c7c66","99845ae85f304e02bc8d7c2062e7f3dc","1f305e00a68d4db39ffd8bd9645516d9","226cba960ddb4bc381c61118304344f0","3a3f7d1c983f4414b4ee6a1140bcf15d"]},"executionInfo":{"status":"ok","timestamp":1630381356600,"user_tz":0,"elapsed":1700,"user":{"displayName":"Matthew Bilton","photoUrl":"","userId":"11777015143175422111"}},"outputId":"d449a2e1-2f94-467a-fac7-0a0e2b9c34aa"},"source":["TOKENIZER = DistilBertTokenizer.from_pretrained(MODEL_NAME, use_fast=True, do_lower_case=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9472d303c871431e9cc3045e44c82548","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f579559fda840bda85e810b14dc9dd2","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"91fb426a45ae49569a3edee4ec5dc218","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1504f065ce0e4cad94113ba38cd7f403","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"O4CQ65Vo2w5Q"},"source":["For reproducability, let's also save this tokenizer:"]},{"cell_type":"code","metadata":{"id":"3twaVOb7TTdk"},"source":["TOKENIZER.save_pretrained('tokenizer')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tVpxiYh4bFDs"},"source":["# Token Lengths\n","\n","## The Problem of Choosing the Maximum Token Length\n","\n","As we previously alluded to, one must *tokenise* a string before passing it to BERT as an input - in other words, a string must be converted to a vector of numerical ID values. Importantly, the length of these numerical ID vectors will depend on the length of the sentence being tokenized. \n","\n","Critically, BERT is unable to process inputs which are longer than 500 tokens, so it's important to figure out if we have any text excerpts which are longer than length. Additionally, the computational cost of using BERT increases *quadratically* with token length (i.e. doubling the token length quadruples the computation time). Both of these facts mean that we must decide on a **maximum token length** to consider - importantly, one must recognise that there is a *trade-off* when deciding on the maximum token length:\n","  - If we pick a token length which is unneccassarily large, our network will be a lot slower to train.\n","  - If we pick a token length which is too small, the network may perform poorly, since there is less information about 'readability' within a shorter pice of text. Indeed, one may actually wonder what we do with the rest of a piece of tokenised text if its length exceeds our 'maximum length' - we'll discuss this issue shortly.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"81xdKNjp8uJQ"},"source":["\n","## Token Length Distribution in our Data\n","\n","To get an understanding of the token lengths corresponding to the text excerpts in our data set, we'll tokenize every input in our data set and then plot the distribution of token lengths:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"1qkpOESmbMCk","executionInfo":{"status":"ok","timestamp":1630359167600,"user_tz":0,"elapsed":12000,"user":{"displayName":"Deborah Bilton","photoUrl":"","userId":"00574069797017848949"}},"outputId":"ca27f45f-aa6f-4e96-877d-5e595ecbb5d1"},"source":["# Initialise list to store tokens in:\n","token_len = []\n","\n","# Tokenize every piece of text in training dataset:\n","for text in features[\"excerpt\"]:\n","  tokens = TOKENIZER.encode(text, truncation=False)\n","  token_len.append(len(tokens))\n","\n","# Plot these token lengths:\n","fig, ax1 = plt.subplots()\n","plt.xlabel('Token Length')\n","sns.kdeplot(data=token_len, ax=ax1, color='r', fill=True)\n","ax2 = ax1.twinx()\n","sns.histplot(data=token_len, discrete=True, ax=ax2)\n","sns.set_style(\"darkgrid\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"0CPeK5Eb5DKM"},"source":["As we can see from the above plot, the token lengths vary between 150 and 300. We'll revisit this issue of choosing a token length when we discuss hyperparameter tuning at the end of this notebook."]},{"cell_type":"markdown","metadata":{"id":"XsbOB9fz8wtU"},"source":["## What Happens if Our Input Text Extracts are Longer than the Maximum Token Length?\n","\n","We previously noted that it's unclear what happens if we choose a maximum token length which is **less than the token length of a particular piece of text**. One way to solve this problem would be to *throw away* all of the token values which occur after our cut-off length; this, however, is wasteful, since we're effectively 'throwing away' a lot of our data. \n","\n","Another solution to this problem would be to simply **slice up** those *training examples longer than the maximum length into multiple new training example*. This means, for example, that if our maximum token length was set to 100, then a text whose tokenisation is 300 values long would be 'sliced' up into *three new training example tokenisations of size 100*. By doing this, we're throwing away a lot less data. \n","\n","This slicing idea makes sense in the context of training our network (i.e. one training example is sliced into multiple training examples), but what happens if we're using our network to *predict* a readability value? How do we go from multiple readability predictions made on sub-string slices of an input into a single readability prediction for the entire string? One simple way to do this would be to compute the **weighted average of the readability scores across all of the token slices**:\n","\n","$\n","\\text{Predicted Readability of Text} = \\sum_{i=0}^{i=N}\\frac{\\text{Length of Token Slice} i}{\\text{Total Token Length of Text}}\\times \\text{Readability of Token Slice }i\n","$\n","\n","By taking this weighted average, we 'pay less attention' to those readability predictions made on shorter text slices (which are probably less accurate than predictions made on longer slices of text). Indeed, we may even want to filter out those slices of text which are **shorter than a specified length** to prevent outlier text slices from unduly swaying our prediction about the entire string."]},{"cell_type":"markdown","metadata":{"id":"CG4aZ2b8Ap4M"},"source":["## Padding to the Maximum Token Length and Attention Masks\n","\n","In theory, it should be fine if a text input has a tokenisation which is shorter than our chosen maximum token length.\n","\n","In practice, however, this **is a problem** - this is because neural networks runs **much faster** if our input data is stored in a **single tensor data structure**. But in order to store all of our inputs in a single tensor, they all need to be of the **same length**. In order to ensure that our input tokens are all of the same length, we need to **pad input tokens shorter than the maximum token length with zeros** (i.e. add zero values to the end of the token). That way, we can be sure all of our input features are the same length.\n","\n","But this padding is entirely artificial, and should not influence the predictions made by the network - how do we tell our BERT network to 'ignore' these zero paddings? It turns out we can use something call an **attention mask** to label those parts of our token input which we want the network to 'pay attention' to (these tokens are labelled with a `1` in the attention mask), and which parts of the token input we don't want the netowkr to 'pay attention' to (these parts are labelled with a `0` in the attention mask). This is all important to know when we finally look at our data pre-processing code."]},{"cell_type":"markdown","metadata":{"id":"0w-kmoQn_XaE"},"source":["## Maximum Token Length as a Hyperparameter\n","\n","At this point, it's worth noting that it would actually be ideal if we would choose the minimum possible token length which still allows our network to perform well - that way, our network can run both **accurately an efficiently**. For this reason, we'll actually explore **multiple different maximum token length values when we consider hyperparameter tuning later on in this notebook**."]},{"cell_type":"markdown","metadata":{"id":"fj8g_YgmiDVU"},"source":["# Pre-Process Data"]},{"cell_type":"markdown","metadata":{"id":"YFL5VhgYeBtQ"},"source":["Let's now split our data into a training dataset (90% of the data) and test dataset (10% of the data). Since our dataset is relatively large here (around 2000 instances), 10% of the dataset should be enough to get a statistically robust estimate of the generalisation error of our network:"]},{"cell_type":"code","metadata":{"id":"r7Wq6KkJeCDI"},"source":["test_frac = 0.1\n","train_features, test_features, train_labels, test_labels \\\n","  = train_test_split(features, labels, test_size=test_frac, random_state=RANDOM_SEED)\n","TRAIN_DATA = {\"features\": train_features, \"labels\": train_labels}\n","TEST_DATA = {\"features\": test_features, \"labels\": test_labels}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"knlx0VAZeCds"},"source":["We now need to process our training and test datasets into tokens which can be 'understood' by BERT. To do this, we'll define the following three major functions:\n","1. `create_train_dataset` - This takes in our **training** data set, tokenises each input, and then slices up these tokenized inputs if they exceed a specified maximum length `max_len` or, alternatively, pads our input tokens if they're shorted than `max_len`. Additionally, it also throws away any text slices which are shorted than `cutoff` in length - for our purposes, we've (relatively arbitrarily) set this `cutoff` value to `30` (i.e. all those token slices with fewer than 30 values will be thrown away). The output of this function are **three tensors**:\n","  - `X_id`, which is a tensor with dimensions `num_extracts × max_num_slices × max_len`, where `num_extracts` is the total number of extracts in our training data, and `max_num_slices` is the *maximum* number of token slices yielded by slicing up an extract accross our entire dataset. Put simply, the `0'th` access is used to access a specific extract, the `1'st` axis corresponds to a particular slice within that extract, and the `2'nd` axis corresponds to a particular token within the slice of that extract. \n","  - `X_mask`, which stores the attention masks corresponding to the tokenised inputs stored in `X_id` - it's dimensions are the same as that of `X_id`\n","  - `y`, which stores the labels of our training examples, and is of dimensions `num_extracts × num_pred`, where `num_pred` is the number of values we're trying to predict (which `= 1` here).\n","2. `create_test_dataset` - This takes in our **test** data set, and basically does the same thing as `create_train_dataset`. The only difference is is that this function also returns an `X_wts` tensor, which is a `num_extracts × max_num_slices` tensor which contains the weights used to compute the weighted average readabilities for each extract by averaging the readability predictions for each slice for that particular extract.\n","3. `tokenise_inputs` - This function, which is called by `create_train_dataset` and `create_test_dataset`, simply tokenises and slices up all of the txt extracts it's given. To achieve this, it calls a helper function called `process_chunks`, adds the start and end ID values to each of these slices, (which are special values used by BERT to recognise the start and end of an input) and pads chunks which are shorter than `max_len`.\n","\n","With those descriptions out of the way, let's now define these functions:"]},{"cell_type":"code","metadata":{"id":"9YE8sj5q_p1w"},"source":["def create_train_dataset(train_data, tokenizer, max_len, cutoff=30, print_flag=False):\n","\n","  train_features, train_labels = train_data[\"features\"], train_data[\"labels\"]\n","\n","  # Convert labels to PyTorch tensors:\n","  y = torch.tensor(train_labels[\"target\"].values, dtype=torch.float32)\n","  y = y.reshape((len(y), 1)) \n","\n","  # Split text into chunks and tokenise those chunks:\n","  X_id, X_mask = tokenise_inputs(train_features, tokenizer, max_len, cutoff)\n","\n","  # Repeat labels for every chunk:\n","  num_chunks_per_ex = X_id.shape[1]\n","  y = y.repeat_interleave(num_chunks_per_ex, 0)\n","\n","  # Reshape X_id and X_mask to ([batch size*num of chunks] × num of mask/id values):\n","  new_shape = (X_id.shape[0]*X_id.shape[1], X_id.shape[2])\n","  X_id, X_mask = X_id.reshape(new_shape), X_mask.reshape(new_shape)\n","\n","  # Create mask to throw away sentence chunks below cutoff length:\n","  chunk_mask = torch.sum(X_mask, axis=1) >= cutoff\n","\n","  # Print output to user:\n","  if print_flag:\n","    # Compute number of (non-empty) sentence chunks and tokens to throw away:\n","    thrown_chunks = (torch.logical_and(torch.logical_not(chunk_mask), torch.sum(X_mask, axis=1)>0)).sum()\n","    total_chunks = (torch.sum(X_mask, axis=1) > 0).sum()\n","    thrown_tokens = X_mask[torch.logical_not(chunk_mask),:].sum()\n","    total_tokens = X_mask.sum()\n","    print(\"Training Data Processing Summary:\")\n","    print(f\"Number of sentence chunks discarded = {torch.sum(thrown_chunks)}/{torch.sum(total_chunks)}\")\n","    print(f\"Number of tokens discarded = {thrown_tokens}/{total_tokens}\")\n","\n","  # Throw away sentence chunks which contain fewer than cutoff ids:\n","  X_id, X_mask, y = X_id[chunk_mask,:], X_mask[chunk_mask,:], y[chunk_mask,:]\n","  \n","  # Place in dataset:\n","  train_dataset = TensorDataset(X_id, X_mask, y)\n","  return train_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iFRa0jwU_wh4"},"source":["def create_test_dataset(test_data, tokenizer, max_len, cutoff=30):\n","\n","  test_features, test_labels = test_data[\"features\"], test_data[\"labels\"]\n","\n","  # Convert labels to PyTorch tensors:\n","  y = torch.tensor(test_labels[\"target\"].values, dtype=torch.float32)\n","  y = y.reshape((len(y), 1)) \n","\n","  # Split text into chunks and tokenise those chunks:\n","  X_id, X_mask = tokenise_inputs(test_features, tokenizer, max_len, cutoff)\n","\n","  # Compute weightings for the sentence chunks:\n","  X_wts = torch.sum(X_mask, axis=2, keepdim=True)/torch.sum(X_mask, axis=(1,2), keepdim=True)\n","  X_wts = X_wts.reshape(X_mask.shape[0:-1])\n","  X_wts = torch.as_tensor(X_wts, dtype=torch.float32)\n","  \n","  # Place tensors into dataset:\n","  test_dataset = TensorDataset(X_id, X_mask, X_wts, y)\n","  return test_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekXKDm3gZ03k"},"source":["def tokenise_inputs(features, tokenizer, max_len, cutoff):\n","\n","  X_id, X_mask = [], []\n","\n","  # Loop over text excerpts:\n","  for text in features[\"excerpt\"]:\n","    id_list, mask_list = [], []\n","\n","    # Tokenize this text:\n","    tokens = tokenizer.encode_plus(text, add_special_tokens=False, return_tensors='pt')\n","\n","    # Split into chunks of length (max_len - 2) - we still need to add the start and end\n","    # IDs to these slices:\n","    ids = tokens[\"input_ids\"][0].split(max_len-2)\n","    masks = tokens[\"attention_mask\"][0].split(max_len-2)\n","\n","    # Process each id_chunk-mask_chunk pair, \n","    for id_chunk, mask_chunk in zip(ids, masks):\n","      id_chunk, mask_chunk = process_chunks(id_chunk, mask_chunk, max_len)\n","      id_list.append(id_chunk), mask_list.append(mask_chunk)\n","\n","    # Add these to our list:  \n","    X_id.append(torch.stack(id_list, dim=0))\n","    X_mask.append(torch.stack(mask_list, dim=0))\n","\n","  # Pad list of IDs and Masks so that they're now both stored in a single tensor\n","  # of dimensions (num_extracts × max_num_slices × max_len):\n","  X_id = pad_sequence(X_id, batch_first=True, padding_value=0)\n","  X_mask = pad_sequence(X_mask, batch_first=True, padding_value=0)\n","  X_id, X_mask = torch.as_tensor(X_id, dtype=torch.int64), torch.as_tensor(X_mask, dtype=torch.int64)\n","  return (X_id, X_mask)\n","\n","def process_chunks(id_chunk, mask_chunk, max_len, start_id=101, end_id=102):\n","\n","  # Add start and stop IDs:\n","  id_chunk = torch.cat([torch.tensor([start_id]), id_chunk, torch.tensor([end_id])])\n","  mask_chunk = torch.cat([torch.ones(1), mask_chunk, torch.ones(1)])\n","\n","  # Pad chunks if required:\n","  if len(id_chunk) < max_len:\n","    pad_len = max_len - len(id_chunk)\n","    id_chunk = torch.cat([id_chunk, torch.zeros(pad_len)])\n","    mask_chunk = torch.cat([mask_chunk, torch.zeros(pad_len)])\n","\n","  return (id_chunk, mask_chunk)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LDB6vlPXS9g4"},"source":["# Create Dataset and DataLoader Objects"]},{"cell_type":"markdown","metadata":{"id":"2ZbNjCwajqRs"},"source":["Now that we have our functions to pre-process our inputs, let's now define a PyTorch Dataset object to store this preprocessed data - note that the behaviour of the dataset object differs depending on whether it's loaded with training data or test data (since test data contains `X_wts`, whereas the training data does not): "]},{"cell_type":"code","metadata":{"id":"d5HagFmDZF68"},"source":["class ReadingDifficultyDataset(data.Dataset):\n","  def __init__(self, dataset, mode):\n","    self.dataset = dataset\n","    self.mode=mode\n","\n","  def __len__(self):\n","    return len(self.dataset)\n","    \n","  def __getitem__(self, idx):\n","    item_i = self.dataset[idx]\n","    if self.mode==\"train\":\n","      item_i = {\"id\": item_i[0],\n","                \"mask\": item_i[1],\n","                \"y\": item_i[2]}\n","    elif self.mode==\"test\":\n","      item_i = {\"id\": item_i[0],\n","                \"mask\": item_i[1],\n","                \"wt\": item_i[2],\n","                \"y\": item_i[3]}\n","    return item_i"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VNc1xeWijsq0"},"source":["We can then define functions to create Dataloader objects corresponding to these Dataset objects:"]},{"cell_type":"code","metadata":{"id":"8AjsTsbCjp6Q"},"source":["BATCH_SIZE = 32\n","\n","def create_train_dataloader(train_data, max_len, tokenizer, batch_size=BATCH_SIZE):\n","  train_dataset = create_train_dataset(train_data, tokenizer, max_len)\n","  train_dataset = ReadingDifficultyDataset(train_dataset, \"train\")\n","  train_dl = data.DataLoader(train_dataset,\n","                             batch_size=batch_size,\n","                             shuffle=True)\n","  return train_dl\n","\n","def create_test_dataloader(test_data, max_len, tokenizer, batch_size=BATCH_SIZE):\n","  test_dataset = create_test_dataset(test_data, tokenizer, max_len)\n","  test_dataset = ReadingDifficultyDataset(test_dataset, \"test\")\n","  test_dl = data.DataLoader(test_dataset,\n","                             batch_size=batch_size,\n","                             shuffle=False)\n","  return test_dl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4e-WHRSkTBOY"},"source":["# Create BERT Model"]},{"cell_type":"markdown","metadata":{"id":"Xd-So4MppZtU"},"source":["We'll now define an object to represent our BERT model - during the initialisation of our model, we simply take note of how many quantities we're predicting and then load in a pre-trained BERT model from the HuggingFace library. Note that the constructor also accepts a `drop_prob` parameter, which can be used to specify the dropout probability of the final dropout layer of the BERT network - this will later be varied as a hyper parameter.\n","\n","We've also defined a `predict` method, which predicts the readability of **test examples by computing the weighted average readability of each extract**."]},{"cell_type":"code","metadata":{"id":"5cc2Hi7KTR8s"},"source":["class BertRegression():\n","  def __init__(self, num_pred, drop_prob):\n","\n","    self.num_pred = num_pred\n","    self.model = \\\n","     DistilBertForSequenceClassification.from_pretrained(MODEL_NAME,\n","                                                         problem_type=\"regression\",\n","                                                         num_labels=num_pred,\n","                                                         seq_classif_dropout=drop_prob,\n","                                                         output_attentions=False,\n","                                                         output_hidden_states=False)\n","\n","  def predict(self, id, att_mask, wt):\n","    \n","    # Check that model is in evaluation mode, not training mode:\n","    if self.model.training:\n","      self.model.eval()\n","    \n","    # Determine input and output shapes:\n","    in_shape = (id.shape[0]*id.shape[1], id.shape[2])\n","    out_shape = id.shape[0:-1] + (self.num_pred,)\n","\n","    # Reshape id and attention mask accordingly:\n","    id, att_mask = id.reshape(in_shape), att_mask.reshape(in_shape)\n","    \n","    # Make prediction using BERT model:\n","    logits = self.model(input_ids=id, attention_mask=att_mask)[\"logits\"]\n","    \n","    # Reshape output:\n","    logits = logits.reshape(out_shape)\n","\n","    # Compute weighted average of slices for each excerpt:\n","    pred = torch.einsum(\"ij,ijk->ik\", wt, torch.nan_to_num(logits))\n","    return pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2CJYE1JNTDfs"},"source":["# Train and Evaluation Loops"]},{"cell_type":"markdown","metadata":{"id":"HBAa_9_678xc"},"source":["Let's now define our functions to run a single epoch of our training loop and our test loop:"]},{"cell_type":"code","metadata":{"id":"664QumlU6IAA"},"source":["def train_epoch(model_obj, data_loader, optimiser, device):\n","  \n","  # Make sure model is in training mode:\n","  model_obj.model.train()\n","\n","  # Initialise loss for this epoch:\n","  total_loss = 0\n","\n","  for d in data_loader:\n","\n","    # Unpack training data:\n","    id = d[\"id\"].to(device)\n","    mask = d[\"mask\"].to(device)\n","    y = d[\"y\"].to(device)\n","\n","    # Produce predictions:\n","    model_obj.model.zero_grad()\n","    output = model_obj.model(input_ids=id, \n","                             attention_mask=mask, \n","                             labels=y)\n","    # Add to loss:\n","    total_loss += output.loss.item()\n","\n","    # Perform backprop:\n","    output.loss.backward()\n","\n","    # Update parameters:\n","    optimiser.step()\n","\n","  # Return average loss across all samples:\n","  return total_loss/len(data_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fcPshuE_6L6A"},"source":["def test_epoch(model_obj, data_loader, test_loss_fun, device):\n","  \n","  # Make sure model is in evaluation mode:\n","  model_obj.model.eval()\n","\n","  # Initialise loss:\n","  total_loss = 0\n","\n","  # Don't need to track gradient since we're not training here:\n","  with torch.no_grad():\n","    for d in data_loader:\n","\n","      # Unpack training data:\n","      id = d[\"id\"].to(device)\n","      mask = d[\"mask\"].to(device)\n","      wt = d[\"wt\"].to(device)\n","      y = d[\"y\"].to(device)\n","\n","      # Produce predictions:\n","      pred = model_obj.predict(id, mask, wt)\n","      loss = test_loss_fun(pred.squeeze(), y.squeeze())\n","\n","      # Add to tests:\n","      total_loss += loss\n","\n","    # Return average test loss:\n","  return total_loss/len(data_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CstsCZh76a5I"},"source":["We can now define a function which trains our BERT model for a **single set of hyperparameters**:"]},{"cell_type":"code","metadata":{"id":"J_fDv7h66duY"},"source":["def train_model(model_obj, optimiser, train_dl, test_dl, num_epoch, device, \n","                print_msg=False, delete_model=True):\n","\n","  # Define print function to use depending on if user wants printed outputs:\n","  my_print = print if print_msg else lambda my_str: None\n","\n","  # Move test loss and model to GPU:\n","  test_loss_fun = nn.MSELoss().to(device)\n","  model_obj.model.to(device)\n","\n","  # Save training and test loss history:\n","  hist = {\"train\": [], \"test\": []}\n","  best_loss = inf\n","\n","  for i in range(num_epoch):\n","\n","    # Print info to user:\n","    my_print(f'Epoch {i+1}/{num_epoch}')\n","    my_print(10*'-')\n","\n","    # Compute training loss:\n","    time_0 = time.time()\n","    train_loss = train_epoch(model_obj, train_dl, optimiser, device)\n","    time_1 = time.time()\n","    my_print(f\"Train loss = {train_loss:.6f}\")\n","    my_print(f\"Training time = {time_1-time_0:.2f} seconds\")\n","\n","    # Compute test loss:\n","    time_0 = time.time()\n","    test_loss = test_epoch(model_obj, test_dl, test_loss_fun, device)\n","    time_1 = time.time()\n","    my_print(f\"Test loss = {test_loss:.6f}\")\n","    my_print(f\"Test time = {time_1-time_0:.2f} seconds\\n\")\n","\n","    # Save best model we've seen thus far:\n","    if test_loss < best_loss:\n","      best_loss = test_loss\n","      model_obj.model.save_pretrained('train_model')\n","\n","    # Record training history:\n","    hist[\"train\"].append(train_loss)\n","    hist[\"test\"].append(test_loss)\n","\n","  # Load best model we saw:\n","  best_model = DistilBertForSequenceClassification.from_pretrained('train_model')\n","  \n","  # Delete model if requested by user:\n","  if delete_model:\n","    shutil.rmtree('train_model')\n","    \n","  return (best_model, best_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4V4_blVf7jvo"},"source":["# Hyperparameter Tuning\n","\n","As we've previously alluded to in this notebook, we need to perform **hyperparameter tuning** to get the best performance out of our finetuned BERT network. In this case, we consider **three hyperparameters to vary**:\n","1. The maximum token length - we've already discussed the rationale behind this decision.\n","2. The dropout probability of the final dropout layer - chosing the right dropout probability betters allows BERT to *generalise* to unseen data.\n","3. The learning rate - this is a 'classic' hyperparameter to vary and can have siginificant impacts on final network performance.\n","\n","Importantly, parameters we choose **not** to vary include:\n","1. The number of epochs - we keep this fixed at `3`, which is a value recommended by most practicioners. Indeed, it's often recommended to **not** train BERT for too many epochs, or else it will begin to 'memorise' one's data too much.\n","2. The batch size, which is fixed at `32` - this was chosen for computational purposes, since batch sizes of `64` took up a significant amount of memory.\n","\n","With this all in mind, let's now define a function which calls our `train_model` function for different combinations of hyperparameters and takes note of the best hyperparameter combination:"]},{"cell_type":"code","metadata":{"id":"bpv68thyXjt4"},"source":["def hp_tuning(hp_list, fixed_params, model_class, train_data, test_data, tokenizer, \n","              num_epoch, device):\n","  \n","  # Create list of dictionaries for all possible hyperparameter combinations:\n","  keys = hp_list.keys()\n","  hp_combos = []\n","  for bundle in itertools.product(*hp_list.values()):\n","    hp_dict = dict(zip(keys, bundle))\n","    hp_combos.append(hp_dict)\n","\n","  # Dictionary to store hyperparameter combinations and losses:\n","  results_dict = {\"hp\": [], \"loss\": [], \"time\": []}\n","\n","  # Loop over each hyperparameter combo:\n","  best_loss = inf\n","  for i, hp in enumerate(hp_combos):\n","\n","    # Start timer:\n","    time_0 = time.time()\n","\n","    # Print progress:\n","    print(f\"Hyper-parameter combination {i+1}/{len(hp_combos)}\")\n","\n","    # Create model:\n","    model = create_model(model_class, hp, fixed_params)\n","\n","    # Create optimiser:\n","    optimiser = create_optimiser(model, hp, fixed_params)\n","\n","    # Create Dataloaders:\n","    train_dl, test_dl = create_dl(train_data, test_data, tokenizer, hp, fixed_params)\n","\n","    # Train model for this choice of hyperparameters:\n","    model, loss = train_model(model, optimiser, train_dl, test_dl, num_epoch, device)\n","\n","    # Stop timer:\n","    time_1 = time.time()\n","\n","    # Store results:\n","    results_dict[\"hp\"].append(hp)\n","    results_dict[\"loss\"].append(loss)\n","    results_dict[\"time\"].append(time_1 - time_0)\n","\n","    # Save model if it's the best we've seen thus far:\n","    if loss < best_loss:\n","      best_loss = loss\n","      best_hp = hp\n","      model.save_pretrained('best_model')\n","      \n","  return (best_hp, best_loss, results_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pup3k_1jmnK0"},"source":["Importantly, the above hyperparameter tuning function calls the following two helper functions to ceate the optimiser, BERT model, and dataloader corresponding to a particular combination of hyperparameters:"]},{"cell_type":"code","metadata":{"id":"rr_AZEPhmilE"},"source":["# Function to create BERT model from hyperparameter values:\n","def create_model(model_class, hyper_params, fixed_params):\n","  model = model_class(fixed_params[\"num_pred\"], \n","                      hyper_params[\"drop_prob\"])\n","  return model\n","\n","# Function to create optimiser from hyperparameter values:\n","def create_optimiser(model_obj, hyper_params, fixed_params):\n","  optimiser =  AdamW(model_obj.model.parameters(), \n","                     lr=hyper_params[\"lr\"],\n","                     weight_decay=1e-3)\n","  return optimiser\n","\n","def create_dl(train_data, test_data, tokenizer, hyper_params, fixed_params):\n","  train_dl = create_train_dataloader(train_data, \n","                                     hyper_params[\"max_len\"], \n","                                     tokenizer)\n","  test_dl  = create_test_dataloader(test_data, \n","                                    hyper_params[\"max_len\"],\n","                                    tokenizer)\n","  return (train_dl, test_dl)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U-gV8Eo8msGc"},"source":["# Training the Network\n","\n","Let's now actually perform our hyperparameter tuning and train our network - in this case, we've chosen three to four values for each of our hyperparameters. The values chosen are based on typically recommended values for hyperparameter tuning. In total, we have 36 hyperparameter combinations; ona Tesla T4 GPU running on Google Colab, it takes around two hours to perform this hyperparameter tuning procedure."]},{"cell_type":"code","metadata":{"id":"6Lf4c7ZRmtlw","colab":{"base_uri":"https://localhost:8080/","height":703,"referenced_widgets":["8b0655fd2a424848bfc3d45ff3e35d4f","9de5818d7f2647889c3754cc2dac929a","8e45b83d7e074972978e5e2fa5ae0bdf","576056fc267a479584dbee4b8f4cc21b","7034090027094112a97a35088231fdd6","26b98202154c4e6f938a43409388af90","2a7eac42b70e4474b0c64ba3ac4e94fe","9b12a71d3d834a1e92bc9e41a4fc7348","8f7e3c3b5cd641e78c5532d012f3f564","d9e6fc59641b424eb2f0a9594e21bdb8","0f57abe5f74c4bbd9c89a47be270a58a"]},"executionInfo":{"status":"ok","timestamp":1630367079800,"user_tz":0,"elapsed":7912200,"user":{"displayName":"Deborah Bilton","photoUrl":"","userId":"00574069797017848949"}},"outputId":"3bb8d57d-e312-4ea5-9af2-ddf8bd619a0c"},"source":["hp_list = {\"lr\": [5e-5, 2.5e-5, 1e-5],\n","           \"drop_prob\": [0.15, 0.2, 0.25],\n","           \"max_len\": [100, 150, 200, 250]}\n","fixed_params = {\"num_pred\": 1}\n","model_class = BertRegression\n","num_epoch = 3\n","best_hp, best_loss, results_dict = \\\n","  hp_tuning(hp_list, fixed_params, model_class, TRAIN_DATA, TEST_DATA, TOKENIZER, num_epoch, device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hyper-parameter combination 1/36\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b0655fd2a424848bfc3d45ff3e35d4f","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Hyper-parameter combination 2/36\n","Hyper-parameter combination 3/36\n","Hyper-parameter combination 4/36\n","Hyper-parameter combination 5/36\n","Hyper-parameter combination 6/36\n","Hyper-parameter combination 7/36\n","Hyper-parameter combination 8/36\n","Hyper-parameter combination 9/36\n","Hyper-parameter combination 10/36\n","Hyper-parameter combination 11/36\n","Hyper-parameter combination 12/36\n","Hyper-parameter combination 13/36\n","Hyper-parameter combination 14/36\n","Hyper-parameter combination 15/36\n","Hyper-parameter combination 16/36\n","Hyper-parameter combination 17/36\n","Hyper-parameter combination 18/36\n","Hyper-parameter combination 19/36\n","Hyper-parameter combination 20/36\n","Hyper-parameter combination 21/36\n","Hyper-parameter combination 22/36\n","Hyper-parameter combination 23/36\n","Hyper-parameter combination 24/36\n","Hyper-parameter combination 25/36\n","Hyper-parameter combination 26/36\n","Hyper-parameter combination 27/36\n","Hyper-parameter combination 28/36\n","Hyper-parameter combination 29/36\n","Hyper-parameter combination 30/36\n","Hyper-parameter combination 31/36\n","Hyper-parameter combination 32/36\n","Hyper-parameter combination 33/36\n","Hyper-parameter combination 34/36\n","Hyper-parameter combination 35/36\n","Hyper-parameter combination 36/36\n"]}]},{"cell_type":"markdown","metadata":{"id":"Mk0w1qhpx-0M"},"source":["Let's now print out the best hyperparameter combination we saw:"]},{"cell_type":"code","metadata":{"id":"ldrgBXVee4V4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630367079800,"user_tz":0,"elapsed":0,"user":{"displayName":"Deborah Bilton","photoUrl":"","userId":"00574069797017848949"}},"outputId":"b762024c-4fb9-46c6-80eb-838b4bf3bb20"},"source":["best_hp"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'drop_prob': 0.15, 'lr': 2.5e-05, 'max_len': 250}"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"zRK6wVphx8kc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630367079800,"user_tz":0,"elapsed":0,"user":{"displayName":"Deborah Bilton","photoUrl":"","userId":"00574069797017848949"}},"outputId":"3c484582-7e58-4ada-f5cb-a521404d3a9e"},"source":["best_loss"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.2358, device='cuda:0')"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"Wz5MMgFaK9i8"},"source":["Let's also print out the entire results dictionary of our hyperparameter tuning:"]},{"cell_type":"code","metadata":{"id":"egU-3KNTx9f0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630367080500,"user_tz":0,"elapsed":700,"user":{"displayName":"Deborah Bilton","photoUrl":"","userId":"00574069797017848949"}},"outputId":"1ecb8d18-b56c-425a-e091-7442997b3844"},"source":["results_dict"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'hp': [{'drop_prob': 0.15, 'lr': 5e-05, 'max_len': 100},\n","  {'drop_prob': 0.15, 'lr': 5e-05, 'max_len': 150},\n","  {'drop_prob': 0.15, 'lr': 5e-05, 'max_len': 200},\n","  {'drop_prob': 0.15, 'lr': 5e-05, 'max_len': 250},\n","  {'drop_prob': 0.2, 'lr': 5e-05, 'max_len': 100},\n","  {'drop_prob': 0.2, 'lr': 5e-05, 'max_len': 150},\n","  {'drop_prob': 0.2, 'lr': 5e-05, 'max_len': 200},\n","  {'drop_prob': 0.2, 'lr': 5e-05, 'max_len': 250},\n","  {'drop_prob': 0.25, 'lr': 5e-05, 'max_len': 100},\n","  {'drop_prob': 0.25, 'lr': 5e-05, 'max_len': 150},\n","  {'drop_prob': 0.25, 'lr': 5e-05, 'max_len': 200},\n","  {'drop_prob': 0.25, 'lr': 5e-05, 'max_len': 250},\n","  {'drop_prob': 0.15, 'lr': 2.5e-05, 'max_len': 100},\n","  {'drop_prob': 0.15, 'lr': 2.5e-05, 'max_len': 150},\n","  {'drop_prob': 0.15, 'lr': 2.5e-05, 'max_len': 200},\n","  {'drop_prob': 0.15, 'lr': 2.5e-05, 'max_len': 250},\n","  {'drop_prob': 0.2, 'lr': 2.5e-05, 'max_len': 100},\n","  {'drop_prob': 0.2, 'lr': 2.5e-05, 'max_len': 150},\n","  {'drop_prob': 0.2, 'lr': 2.5e-05, 'max_len': 200},\n","  {'drop_prob': 0.2, 'lr': 2.5e-05, 'max_len': 250},\n","  {'drop_prob': 0.25, 'lr': 2.5e-05, 'max_len': 100},\n","  {'drop_prob': 0.25, 'lr': 2.5e-05, 'max_len': 150},\n","  {'drop_prob': 0.25, 'lr': 2.5e-05, 'max_len': 200},\n","  {'drop_prob': 0.25, 'lr': 2.5e-05, 'max_len': 250},\n","  {'drop_prob': 0.15, 'lr': 1e-05, 'max_len': 100},\n","  {'drop_prob': 0.15, 'lr': 1e-05, 'max_len': 150},\n","  {'drop_prob': 0.15, 'lr': 1e-05, 'max_len': 200},\n","  {'drop_prob': 0.15, 'lr': 1e-05, 'max_len': 250},\n","  {'drop_prob': 0.2, 'lr': 1e-05, 'max_len': 100},\n","  {'drop_prob': 0.2, 'lr': 1e-05, 'max_len': 150},\n","  {'drop_prob': 0.2, 'lr': 1e-05, 'max_len': 200},\n","  {'drop_prob': 0.2, 'lr': 1e-05, 'max_len': 250},\n","  {'drop_prob': 0.25, 'lr': 1e-05, 'max_len': 100},\n","  {'drop_prob': 0.25, 'lr': 1e-05, 'max_len': 150},\n","  {'drop_prob': 0.25, 'lr': 1e-05, 'max_len': 200},\n","  {'drop_prob': 0.25, 'lr': 1e-05, 'max_len': 250}],\n"," 'loss': [tensor(0.2819, device='cuda:0'),\n","  tensor(0.3084, device='cuda:0'),\n","  tensor(0.2886, device='cuda:0'),\n","  tensor(0.2961, device='cuda:0'),\n","  tensor(0.3120, device='cuda:0'),\n","  tensor(0.2938, device='cuda:0'),\n","  tensor(0.3031, device='cuda:0'),\n","  tensor(0.2730, device='cuda:0'),\n","  tensor(0.3391, device='cuda:0'),\n","  tensor(0.3043, device='cuda:0'),\n","  tensor(0.2841, device='cuda:0'),\n","  tensor(0.2400, device='cuda:0'),\n","  tensor(0.2987, device='cuda:0'),\n","  tensor(0.2814, device='cuda:0'),\n","  tensor(0.2687, device='cuda:0'),\n","  tensor(0.2358, device='cuda:0'),\n","  tensor(0.3106, device='cuda:0'),\n","  tensor(0.2657, device='cuda:0'),\n","  tensor(0.2725, device='cuda:0'),\n","  tensor(0.2788, device='cuda:0'),\n","  tensor(0.2919, device='cuda:0'),\n","  tensor(0.2857, device='cuda:0'),\n","  tensor(0.3013, device='cuda:0'),\n","  tensor(0.2590, device='cuda:0'),\n","  tensor(0.2911, device='cuda:0'),\n","  tensor(0.2514, device='cuda:0'),\n","  tensor(0.2789, device='cuda:0'),\n","  tensor(0.3174, device='cuda:0'),\n","  tensor(0.3072, device='cuda:0'),\n","  tensor(0.2565, device='cuda:0'),\n","  tensor(0.2605, device='cuda:0'),\n","  tensor(0.2647, device='cuda:0'),\n","  tensor(0.2763, device='cuda:0'),\n","  tensor(0.2749, device='cuda:0'),\n","  tensor(0.2718, device='cuda:0'),\n","  tensor(0.2834, device='cuda:0')],\n"," 'time': [195.85441040992737,\n","  231.70078086853027,\n","  228.43054747581482,\n","  214.9869887828827,\n","  195.53757667541504,\n","  236.12435913085938,\n","  229.41183853149414,\n","  215.0048017501831,\n","  195.76926612854004,\n","  236.9454197883606,\n","  231.9814579486847,\n","  216.057847738266,\n","  195.8500316143036,\n","  237.16799449920654,\n","  229.56371474266052,\n","  215.7665855884552,\n","  196.9466392993927,\n","  238.06777334213257,\n","  230.13424730300903,\n","  215.62041401863098,\n","  197.30783200263977,\n","  235.3559272289276,\n","  229.52934622764587,\n","  215.25695037841797,\n","  196.5856544971466,\n","  237.8142011165619,\n","  231.18381237983704,\n","  217.07421493530273,\n","  195.83931159973145,\n","  237.6626501083374,\n","  231.18570613861084,\n","  215.989666223526,\n","  197.0068199634552,\n","  236.6607129573822,\n","  231.1072962284088,\n","  215.72562193870544]}"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"hOMm7ihKLCQg"},"source":["# Kaggle Link to Best Model\n","\n","The best model trained by this notebook can be accessed as a [dataset on Kaggle](https://www.kaggle.com/matthewabilton/finetuned-bert-readability)."]}]}